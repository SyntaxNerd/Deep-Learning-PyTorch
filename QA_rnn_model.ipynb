{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWD09+ZUoysv9aVvQt8XKj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ira4WuW00JkS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4sxAtSwFjKD",
        "outputId": "76b51725-6c6a-40c1-f163-7b13e779ad20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DWxrKXiHGdS",
        "outputId": "7b60f0ca-4dd2-48ad-9366-b132bf8cb5cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/Datasets/100_Unique_QA_Dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "WxJsrnoUFvf2",
        "outputId": "5621b0cf-ce64-4ace-f860-823523ac3e88"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          question      answer\n",
              "0                   What is the capital of France?       Paris\n",
              "1                  What is the capital of Germany?      Berlin\n",
              "2               Who wrote 'To Kill a Mockingbird'?  Harper-Lee\n",
              "3  What is the largest planet in our solar system?     Jupiter\n",
              "4   What is the boiling point of water in Celsius?         100"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c7335aa-da00-4720-814b-3682dca78c2e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the capital of France?</td>\n",
              "      <td>Paris</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the capital of Germany?</td>\n",
              "      <td>Berlin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
              "      <td>Harper-Lee</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the largest planet in our solar system?</td>\n",
              "      <td>Jupiter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the boiling point of water in Celsius?</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c7335aa-da00-4720-814b-3682dca78c2e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3c7335aa-da00-4720-814b-3682dca78c2e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3c7335aa-da00-4720-814b-3682dca78c2e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 90,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 90,\n        \"samples\": [\n          \"What is the currency of China?\",\n          \"What is the capital of Australia?\",\n          \"Who discovered electricity?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 85,\n        \"samples\": [\n          \"ChristopherColumbus\",\n          \"Paris\",\n          \"Christmas\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "Lh02OvAbF96F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize(\"What is the capital of France?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf_Q1bUYHBtb",
        "outputId": "1fb56e44-3a99-43a6-d220-db8aac00f541"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what', 'is', 'the', 'capital', 'of', 'france', '?']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {\"<UNK>\": 0}"
      ],
      "metadata": {
        "id": "-J-sCV4eHE2l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(row):\n",
        "    tokenized_question = tokenize(row[\"question\"])\n",
        "    tokenized_answer = tokenize(row[\"answer\"])\n",
        "    merged_tokens = tokenized_question + tokenized_answer\n",
        "\n",
        "    for token in merged_tokens:\n",
        "        if token not in vocab:\n",
        "            vocab[token] = len(vocab)"
      ],
      "metadata": {
        "id": "s7bBri3VIUxT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.apply(build_vocab, axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "916g4bE5IaDe",
        "outputId": "ecbefdc7-0c9b-4f0d-f463-857817fa28a4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     None\n",
              "1     None\n",
              "2     None\n",
              "3     None\n",
              "4     None\n",
              "      ... \n",
              "85    None\n",
              "86    None\n",
              "87    None\n",
              "88    None\n",
              "89    None\n",
              "Length: 90, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90 rows Ã— 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NQgWxGIKIg41",
        "outputId": "bfbdd45d-f0d0-4c3a-b45e-6a8ca2f9db3b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<UNK>': 0,\n",
              " 'what': 1,\n",
              " 'is': 2,\n",
              " 'the': 3,\n",
              " 'capital': 4,\n",
              " 'of': 5,\n",
              " 'france': 6,\n",
              " '?': 7,\n",
              " 'paris': 8,\n",
              " 'germany': 9,\n",
              " 'berlin': 10,\n",
              " 'who': 11,\n",
              " 'wrote': 12,\n",
              " \"'to\": 13,\n",
              " 'kill': 14,\n",
              " 'a': 15,\n",
              " 'mockingbird': 16,\n",
              " \"'\": 17,\n",
              " 'harper-lee': 18,\n",
              " 'largest': 19,\n",
              " 'planet': 20,\n",
              " 'in': 21,\n",
              " 'our': 22,\n",
              " 'solar': 23,\n",
              " 'system': 24,\n",
              " 'jupiter': 25,\n",
              " 'boiling': 26,\n",
              " 'point': 27,\n",
              " 'water': 28,\n",
              " 'celsius': 29,\n",
              " '100': 30,\n",
              " 'painted': 31,\n",
              " 'mona': 32,\n",
              " 'lisa': 33,\n",
              " 'leonardo-da-vinci': 34,\n",
              " 'square': 35,\n",
              " 'root': 36,\n",
              " '64': 37,\n",
              " '8': 38,\n",
              " 'chemical': 39,\n",
              " 'symbol': 40,\n",
              " 'for': 41,\n",
              " 'gold': 42,\n",
              " 'au': 43,\n",
              " 'which': 44,\n",
              " 'year': 45,\n",
              " 'did': 46,\n",
              " 'world': 47,\n",
              " 'war': 48,\n",
              " 'ii': 49,\n",
              " 'end': 50,\n",
              " '1945': 51,\n",
              " 'longest': 52,\n",
              " 'river': 53,\n",
              " 'nile': 54,\n",
              " 'japan': 55,\n",
              " 'tokyo': 56,\n",
              " 'developed': 57,\n",
              " 'theory': 58,\n",
              " 'relativity': 59,\n",
              " 'albert-einstein': 60,\n",
              " 'freezing': 61,\n",
              " 'fahrenheit': 62,\n",
              " '32': 63,\n",
              " 'known': 64,\n",
              " 'as': 65,\n",
              " 'red': 66,\n",
              " 'mars': 67,\n",
              " 'author': 68,\n",
              " \"'1984\": 69,\n",
              " 'george-orwell': 70,\n",
              " 'currency': 71,\n",
              " 'united': 72,\n",
              " 'kingdom': 73,\n",
              " 'pound': 74,\n",
              " 'india': 75,\n",
              " 'delhi': 76,\n",
              " 'discovered': 77,\n",
              " 'gravity': 78,\n",
              " 'newton': 79,\n",
              " 'how': 80,\n",
              " 'many': 81,\n",
              " 'continents': 82,\n",
              " 'are': 83,\n",
              " 'there': 84,\n",
              " 'on': 85,\n",
              " 'earth': 86,\n",
              " '7': 87,\n",
              " 'gas': 88,\n",
              " 'do': 89,\n",
              " 'plants': 90,\n",
              " 'use': 91,\n",
              " 'photosynthesis': 92,\n",
              " 'co2': 93,\n",
              " 'smallest': 94,\n",
              " 'prime': 95,\n",
              " 'number': 96,\n",
              " '2': 97,\n",
              " 'invented': 98,\n",
              " 'telephone': 99,\n",
              " 'alexander-graham-bell': 100,\n",
              " 'australia': 101,\n",
              " 'canberra': 102,\n",
              " 'ocean': 103,\n",
              " 'pacific-ocean': 104,\n",
              " 'speed': 105,\n",
              " 'light': 106,\n",
              " 'vacuum': 107,\n",
              " '299,792,458m/s': 108,\n",
              " 'language': 109,\n",
              " 'spoken': 110,\n",
              " 'brazil': 111,\n",
              " 'portuguese': 112,\n",
              " 'penicillin': 113,\n",
              " 'alexander-fleming': 114,\n",
              " 'canada': 115,\n",
              " 'ottawa': 116,\n",
              " 'mammal': 117,\n",
              " 'whale': 118,\n",
              " 'element': 119,\n",
              " 'has': 120,\n",
              " 'atomic': 121,\n",
              " '1': 122,\n",
              " 'hydrogen': 123,\n",
              " 'tallest': 124,\n",
              " 'mountain': 125,\n",
              " 'everest': 126,\n",
              " 'city': 127,\n",
              " 'big': 128,\n",
              " 'apple': 129,\n",
              " 'newyork': 130,\n",
              " 'planets': 131,\n",
              " \"'starry\": 132,\n",
              " 'night': 133,\n",
              " 'vangogh': 134,\n",
              " 'formula': 135,\n",
              " 'h2o': 136,\n",
              " 'italy': 137,\n",
              " 'rome': 138,\n",
              " 'country': 139,\n",
              " 'famous': 140,\n",
              " 'sushi': 141,\n",
              " 'was': 142,\n",
              " 'first': 143,\n",
              " 'person': 144,\n",
              " 'to': 145,\n",
              " 'step': 146,\n",
              " 'moon': 147,\n",
              " 'armstrong': 148,\n",
              " 'main': 149,\n",
              " 'ingredient': 150,\n",
              " 'guacamole': 151,\n",
              " 'avocado': 152,\n",
              " 'sides': 153,\n",
              " 'does': 154,\n",
              " 'hexagon': 155,\n",
              " 'have': 156,\n",
              " '6': 157,\n",
              " 'china': 158,\n",
              " 'yuan': 159,\n",
              " \"'pride\": 160,\n",
              " 'and': 161,\n",
              " 'prejudice': 162,\n",
              " 'jane-austen': 163,\n",
              " 'iron': 164,\n",
              " 'fe': 165,\n",
              " 'hardest': 166,\n",
              " 'natural': 167,\n",
              " 'substance': 168,\n",
              " 'diamond': 169,\n",
              " 'continent': 170,\n",
              " 'by': 171,\n",
              " 'area': 172,\n",
              " 'asia': 173,\n",
              " 'president': 174,\n",
              " 'states': 175,\n",
              " 'george-washington': 176,\n",
              " 'bird': 177,\n",
              " 'its': 178,\n",
              " 'ability': 179,\n",
              " 'mimic': 180,\n",
              " 'sounds': 181,\n",
              " 'parrot': 182,\n",
              " 'longest-running': 183,\n",
              " 'animated': 184,\n",
              " 'tv': 185,\n",
              " 'show': 186,\n",
              " 'simpsons': 187,\n",
              " 'vaticancity': 188,\n",
              " 'most': 189,\n",
              " 'moons': 190,\n",
              " 'saturn': 191,\n",
              " \"'romeo\": 192,\n",
              " 'juliet': 193,\n",
              " 'shakespeare': 194,\n",
              " \"'s\": 195,\n",
              " 'atmosphere': 196,\n",
              " 'nitrogen': 197,\n",
              " 'bones': 198,\n",
              " 'adult': 199,\n",
              " 'human': 200,\n",
              " 'body': 201,\n",
              " '206': 202,\n",
              " 'metal': 203,\n",
              " 'liquid': 204,\n",
              " 'at': 205,\n",
              " 'room': 206,\n",
              " 'temperature': 207,\n",
              " 'mercury': 208,\n",
              " 'russia': 209,\n",
              " 'moscow': 210,\n",
              " 'electricity': 211,\n",
              " 'benjamin-franklin': 212,\n",
              " 'second-largest': 213,\n",
              " 'land': 214,\n",
              " 'color': 215,\n",
              " 'ripe': 216,\n",
              " 'banana': 217,\n",
              " 'yellow': 218,\n",
              " 'month': 219,\n",
              " '28': 220,\n",
              " 'days': 221,\n",
              " 'common': 222,\n",
              " 'february': 223,\n",
              " 'study': 224,\n",
              " 'living': 225,\n",
              " 'organisms': 226,\n",
              " 'called': 227,\n",
              " 'biology': 228,\n",
              " 'home': 229,\n",
              " 'great': 230,\n",
              " 'wall': 231,\n",
              " 'bees': 232,\n",
              " 'collect': 233,\n",
              " 'from': 234,\n",
              " 'flowers': 235,\n",
              " 'nectar': 236,\n",
              " 'opposite': 237,\n",
              " \"'day\": 238,\n",
              " 'south': 239,\n",
              " 'korea': 240,\n",
              " 'seoul': 241,\n",
              " 'bulb': 242,\n",
              " 'edison': 243,\n",
              " 'humans': 244,\n",
              " 'breathe': 245,\n",
              " 'survival': 246,\n",
              " 'oxygen': 247,\n",
              " '144': 248,\n",
              " '12': 249,\n",
              " 'pyramids': 250,\n",
              " 'giza': 251,\n",
              " 'egypt': 252,\n",
              " 'sea': 253,\n",
              " 'creature': 254,\n",
              " 'eight': 255,\n",
              " 'arms': 256,\n",
              " 'octopus': 257,\n",
              " 'holiday': 258,\n",
              " 'celebrated': 259,\n",
              " 'december': 260,\n",
              " '25': 261,\n",
              " 'christmas': 262,\n",
              " 'yen': 263,\n",
              " 'legs': 264,\n",
              " 'spider': 265,\n",
              " 'sport': 266,\n",
              " 'uses': 267,\n",
              " 'net': 268,\n",
              " ',': 269,\n",
              " 'ball': 270,\n",
              " 'hoop': 271,\n",
              " 'basketball': 272,\n",
              " 'kangaroos': 273,\n",
              " 'female': 274,\n",
              " 'minister': 275,\n",
              " 'uk': 276,\n",
              " 'margaretthatcher': 277,\n",
              " 'fastest': 278,\n",
              " 'animal': 279,\n",
              " 'cheetah': 280,\n",
              " 'periodic': 281,\n",
              " 'table': 282,\n",
              " 'spain': 283,\n",
              " 'madrid': 284,\n",
              " 'closest': 285,\n",
              " 'sun': 286,\n",
              " 'father': 287,\n",
              " 'computers': 288,\n",
              " 'charlesbabbage': 289,\n",
              " 'mexico': 290,\n",
              " 'mexicocity': 291,\n",
              " 'colors': 292,\n",
              " 'rainbow': 293,\n",
              " 'musical': 294,\n",
              " 'instrument': 295,\n",
              " 'black': 296,\n",
              " 'white': 297,\n",
              " 'keys': 298,\n",
              " 'piano': 299,\n",
              " 'americas': 300,\n",
              " '1492': 301,\n",
              " 'christophercolumbus': 302,\n",
              " 'disney': 303,\n",
              " 'character': 304,\n",
              " 'long': 305,\n",
              " 'nose': 306,\n",
              " 'grows': 307,\n",
              " 'it': 308,\n",
              " 'when': 309,\n",
              " 'lying': 310,\n",
              " 'pinocchio': 311,\n",
              " 'directed': 312,\n",
              " 'movie': 313,\n",
              " \"'titanic\": 314,\n",
              " 'jamescameron': 315,\n",
              " 'superhero': 316,\n",
              " 'also': 317,\n",
              " 'dark': 318,\n",
              " 'knight': 319,\n",
              " 'batman': 320,\n",
              " 'brasilia': 321,\n",
              " 'fruit': 322,\n",
              " 'king': 323,\n",
              " 'fruits': 324,\n",
              " 'mango': 325,\n",
              " 'eiffel': 326,\n",
              " 'tower': 327}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_ATuSdwOswh",
        "outputId": "5974a5bf-0ecb-43df-f1b6-f88481369df1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "328"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_indices(text, vocab):\n",
        "    indexed_text = []\n",
        "    for token in tokenize(text):\n",
        "        if token in vocab:\n",
        "            indexed_text.append(vocab[token])\n",
        "        else:\n",
        "            indexed_text.append(vocab[\"<UNK>\"])\n",
        "\n",
        "    return indexed_text"
      ],
      "metadata": {
        "id": "9Al8YMfPO7G5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_indices(\"What is the capital of France?\", vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wupwtx6QtwM",
        "outputId": "a0009070-2ae1-4d28-89a3-74c6382e0437"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "GzKC_0MuQxTK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QADataset(Dataset):\n",
        "    def __init__(self, df, vocab):\n",
        "        self.df = df\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        numerical_question = text_to_indices(self.df.iloc[index][\"question\"], self.vocab)\n",
        "        numerical_answer = text_to_indices(self.df.iloc[index][\"answer\"], self.vocab)\n",
        "\n",
        "        return torch.tensor(numerical_question), torch.tensor(numerical_answer)"
      ],
      "metadata": {
        "id": "j5VWx1NERczR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = QADataset(df, vocab)"
      ],
      "metadata": {
        "id": "HFJwxkYuW1va"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size = 1, shuffle = True)"
      ],
      "metadata": {
        "id": "tnPph4qvXCwz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for question, answer in dataloader:\n",
        "    print(question, answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2redYWAKXqwo",
        "outputId": "935b6c81-14bd-4882-b3b2-5aa6175238a1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[11, 57,  3, 58,  5, 59,  7]]) tensor([[60]])\n",
            "tensor([[  1,   2,   3,   4,   5, 115,   7]]) tensor([[116]])\n",
            "tensor([[ 1,  2,  3, 61, 27,  5, 28, 21, 62,  7]]) tensor([[63]])\n",
            "tensor([[  1,   2,   3, 143, 119,  85,   3, 281, 282,   7]]) tensor([[123]])\n",
            "tensor([[ 44, 139, 120,   3, 250,   5, 251,   7]]) tensor([[252]])\n",
            "tensor([[ 11, 312,   3, 313, 314,  17,   7]]) tensor([[315]])\n",
            "tensor([[ 44, 119, 120,   3, 121,  96, 122,   7]]) tensor([[123]])\n",
            "tensor([[ 80,  81, 198,  83,  21,   3, 199, 200, 201,   7]]) tensor([[202]])\n",
            "tensor([[ 1,  2,  3,  4,  5, 55,  7]]) tensor([[56]])\n",
            "tensor([[ 1,  2,  3, 71,  5, 55,  7]]) tensor([[263]])\n",
            "tensor([[11, 77, 78,  7]]) tensor([[79]])\n",
            "tensor([[ 44, 294, 295, 120, 296, 161, 297, 298,   7]]) tensor([[299]])\n",
            "tensor([[ 80,  81, 292,  83,  21,  15, 293,   7]]) tensor([[87]])\n",
            "tensor([[  1,   2,   3,  39,  40,  41, 164,   7]]) tensor([[165]])\n",
            "tensor([[ 44,  88,  89, 244, 245,  21,  41, 246,   7]]) tensor([[247]])\n",
            "tensor([[ 44, 170,   2,   3,  19, 171, 172,   7]]) tensor([[173]])\n",
            "tensor([[ 44, 303, 304, 120,  15, 305, 306, 161, 307, 308, 309, 310,   7]]) tensor([[311]])\n",
            "tensor([[  1,   2,   3, 237,   5, 238,  17,   7]]) tensor([[133]])\n",
            "tensor([[ 44,   2,   3, 213, 139, 171, 214, 172,   7]]) tensor([[115]])\n",
            "tensor([[11, 31,  3, 32, 33,  7]]) tensor([[34]])\n",
            "tensor([[ 44, 258,   2, 259,  85, 260, 261,   7]]) tensor([[262]])\n",
            "tensor([[  1,   2,   3, 105,   5, 106,  21, 107,   7]]) tensor([[108]])\n",
            "tensor([[ 1,  2,  3, 35, 36,  5, 37,  7]]) tensor([[38]])\n",
            "tensor([[  1,   2,   3,  19, 117,  85,  86,   7]]) tensor([[118]])\n",
            "tensor([[ 44, 109,   2, 110,  21, 111,   7]]) tensor([[112]])\n",
            "tensor([[ 11, 142,   3, 143, 174,   5,   3,  72, 175,   7]]) tensor([[176]])\n",
            "tensor([[  1,   2,   3, 149, 150,  21, 151,   7]]) tensor([[152]])\n",
            "tensor([[ 44, 139,   2, 140,  41, 141,   7]]) tensor([[55]])\n",
            "tensor([[  1,   2,   3,  35,  36,   5, 248,   7]]) tensor([[249]])\n",
            "tensor([[ 11,  12, 192, 161, 193,  17,   7]]) tensor([[194]])\n",
            "tensor([[  1,   2,   3, 166, 167, 168,  85,  86,   7]]) tensor([[169]])\n",
            "tensor([[44, 20,  2, 64, 65,  3, 66, 20,  7]]) tensor([[67]])\n",
            "tensor([[  1,  89, 232, 233, 234, 235,   7]]) tensor([[236]])\n",
            "tensor([[ 44, 253, 254, 120, 255, 256,   7]]) tensor([[257]])\n",
            "tensor([[  1,   2,   3,   4,   5, 137,   7]]) tensor([[138]])\n",
            "tensor([[  1,   2,   3,  39, 135,   5,  28,   7]]) tensor([[136]])\n",
            "tensor([[11,  2,  3, 68,  5, 69, 17,  7]]) tensor([[70]])\n",
            "tensor([[  1,   2,   3,   4,   5, 209,   7]]) tensor([[210]])\n",
            "tensor([[  1,   2,   3, 224,   5, 225, 226, 227,   7]]) tensor([[228]])\n",
            "tensor([[ 44,  20,   2,   3, 285, 145,   3, 286,   7]]) tensor([[208]])\n",
            "tensor([[ 1,  2,  3, 19, 20, 21, 22, 23, 24,  7]]) tensor([[25]])\n",
            "tensor([[  1,   2,   3, 183, 184, 185, 186,   7]]) tensor([[187]])\n",
            "tensor([[ 11,  12, 160, 161, 162,  17,   7]]) tensor([[163]])\n",
            "tensor([[1, 2, 3, 4, 5, 6, 7]]) tensor([[8]])\n",
            "tensor([[ 1,  2,  3, 39, 40, 41, 42,  7]]) tensor([[43]])\n",
            "tensor([[ 44, 139,   2, 140,  41, 178, 273,   7]]) tensor([[101]])\n",
            "tensor([[ 1,  2,  3, 52, 53, 21,  3, 47,  7]]) tensor([[54]])\n",
            "tensor([[ 44, 177,   2,  64,  41, 178, 179, 145, 180, 181,   7]]) tensor([[182]])\n",
            "tensor([[1, 2, 3, 4, 5, 9, 7]]) tensor([[10]])\n",
            "tensor([[  1,   2,   3,   4,   5, 111,   7]]) tensor([[321]])\n",
            "tensor([[ 44, 322,   2,  64,  65,   3, 323,   5, 324,   7]]) tensor([[325]])\n",
            "tensor([[ 44,   2,   3, 278, 214, 279,   7]]) tensor([[280]])\n",
            "tensor([[11, 12, 13, 14, 15, 16, 17,  7]]) tensor([[18]])\n",
            "tensor([[  1,   2,   3, 149,  88,  21,  86, 195, 196,   7]]) tensor([[197]])\n",
            "tensor([[  1,   2,   3,   4,   5, 283,   7]]) tensor([[284]])\n",
            "tensor([[ 80,  81, 131,  83,  21,   3,  23,  24,   7]]) tensor([[38]])\n",
            "tensor([[ 44,  20, 120,   3, 189, 190,   7]]) tensor([[191]])\n",
            "tensor([[80, 81, 82, 83, 84, 85, 86,  7]]) tensor([[87]])\n",
            "tensor([[  1,   2,   3,   4,   5, 101,   7]]) tensor([[102]])\n",
            "tensor([[ 11,   2,  64,  65,   3, 287,   5, 288,   7]]) tensor([[289]])\n",
            "tensor([[ 44, 127,   2,  64,  65,   3, 128, 129,   7]]) tensor([[130]])\n",
            "tensor([[ 44, 103,   2,   3,  19,   7]]) tensor([[104]])\n",
            "tensor([[ 44, 203,   2,  15, 204, 205, 206, 207,   7]]) tensor([[208]])\n",
            "tensor([[  1,   2,   3,  94, 139,  21,   3,  47,   7]]) tensor([[188]])\n",
            "tensor([[ 44, 139,   2,  64,  41,   3, 326, 327,   7]]) tensor([[6]])\n",
            "tensor([[ 11,  98,   3, 106, 242,   7]]) tensor([[243]])\n",
            "tensor([[44, 45, 46, 47, 48, 49, 50,  7]]) tensor([[51]])\n",
            "tensor([[  1,   2,   3, 124, 125,  21,   3,  47,   7]]) tensor([[126]])\n",
            "tensor([[ 1,  2,  3, 94, 95, 96,  7]]) tensor([[97]])\n",
            "tensor([[ 44, 266, 267,  15, 268, 269, 270, 269, 161, 271,   7]]) tensor([[272]])\n",
            "tensor([[ 11,  77, 211,   7]]) tensor([[212]])\n",
            "tensor([[ 11, 142,   3, 143, 274,  95, 275,   5,   3, 276,   7]]) tensor([[277]])\n",
            "tensor([[44, 88, 89, 90, 91, 41, 92,  7]]) tensor([[93]])\n",
            "tensor([[  1,   2,   3,  71,   5, 158,   7]]) tensor([[159]])\n",
            "tensor([[11, 98,  3, 99,  7]]) tensor([[100]])\n",
            "tensor([[ 44, 139,   2, 229, 145,   3, 230, 231,   7]]) tensor([[158]])\n",
            "tensor([[  1,   2,   3,   4,   5, 290,   7]]) tensor([[291]])\n",
            "tensor([[ 1,  2,  3,  4,  5, 75,  7]]) tensor([[76]])\n",
            "tensor([[ 1,  2,  3, 26, 27,  5, 28, 21, 29,  7]]) tensor([[30]])\n",
            "tensor([[ 80,  81, 264, 154,  15, 265, 156,   7]]) tensor([[38]])\n",
            "tensor([[ 80,  81, 153, 154,  15, 155, 156,   7]]) tensor([[157]])\n",
            "tensor([[ 44, 219, 120, 220, 221,  21,  15, 222,  45,   7]]) tensor([[223]])\n",
            "tensor([[ 11,  31, 132, 133,  17,   7]]) tensor([[134]])\n",
            "tensor([[ 44, 316,   2, 317,  64,  65,   3, 318, 319,   7]]) tensor([[320]])\n",
            "tensor([[ 11,  77,   3, 300,  21, 301,   7]]) tensor([[302]])\n",
            "tensor([[  1,   2,   3, 215,   5,  15, 216, 217,   7]]) tensor([[218]])\n",
            "tensor([[ 11,  77, 113,   7]]) tensor([[114]])\n",
            "tensor([[ 1,  2,  3, 71,  5,  3, 72, 73,  7]]) tensor([[74]])\n",
            "tensor([[  1,   2,   3,   4,   5, 239, 240,   7]]) tensor([[241]])\n",
            "tensor([[ 11, 142,   3, 143, 144, 145, 146,  85,   3, 147,   7]]) tensor([[148]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "U8tOdKIDYi1w"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim = 50)\n",
        "        self.rnn = nn.RNN(50, 64, batch_first = True)\n",
        "        self.linear = nn.Linear(64, vocab_size)\n",
        "\n",
        "    def forward(self, question):\n",
        "        embeddings_question = self.embedding(question)\n",
        "        hidden_state, final_state = self.rnn(embeddings_question)\n",
        "        output = self.linear(final_state).squeeze(0)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "MYqqH6QdXuOG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "lr = 0.001\n",
        "model = RNN(len(vocab))"
      ],
      "metadata": {
        "id": "GDswANtjdXzj"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = lr)"
      ],
      "metadata": {
        "id": "U80zxxi0dxFT"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    total_epochs_loss = 0\n",
        "    for question, answer in dataloader:\n",
        "        output = model(question)\n",
        "        loss = criterion(output, answer[0])\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_epochs_loss += loss.item()\n",
        "    print(f\"Epoch: {epoch + 1}, Loss: {total_epochs_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UESL2K36dps3",
        "outputId": "acd30bc3-189b-4150-df64-d7a5078d8f9f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 530.9022798538208\n",
            "Epoch: 2, Loss: 431.1832616329193\n",
            "Epoch: 3, Loss: 377.97755670547485\n",
            "Epoch: 4, Loss: 335.35286712646484\n",
            "Epoch: 5, Loss: 292.5800337791443\n",
            "Epoch: 6, Loss: 251.60721600055695\n",
            "Epoch: 7, Loss: 212.40984916687012\n",
            "Epoch: 8, Loss: 177.58302557468414\n",
            "Epoch: 9, Loss: 144.49323672056198\n",
            "Epoch: 10, Loss: 117.03786534070969\n",
            "Epoch: 11, Loss: 93.68509736657143\n",
            "Epoch: 12, Loss: 74.6953085064888\n",
            "Epoch: 13, Loss: 60.30694195628166\n",
            "Epoch: 14, Loss: 48.63182473182678\n",
            "Epoch: 15, Loss: 39.29434812068939\n",
            "Epoch: 16, Loss: 32.46164807677269\n",
            "Epoch: 17, Loss: 26.731629975140095\n",
            "Epoch: 18, Loss: 22.40000295639038\n",
            "Epoch: 19, Loss: 19.003661312162876\n",
            "Epoch: 20, Loss: 16.302626214921474\n",
            "Epoch: 21, Loss: 14.092831756919622\n",
            "Epoch: 22, Loss: 12.378021985292435\n",
            "Epoch: 23, Loss: 10.856917068362236\n",
            "Epoch: 24, Loss: 9.672311279922724\n",
            "Epoch: 25, Loss: 8.57206992059946\n",
            "Epoch: 26, Loss: 7.709961157292128\n",
            "Epoch: 27, Loss: 6.964719507843256\n",
            "Epoch: 28, Loss: 6.323299324139953\n",
            "Epoch: 29, Loss: 5.7442640867084265\n",
            "Epoch: 30, Loss: 5.238130567595363\n",
            "Epoch: 31, Loss: 4.7995812725275755\n",
            "Epoch: 32, Loss: 4.40426885895431\n",
            "Epoch: 33, Loss: 4.058252299204469\n",
            "Epoch: 34, Loss: 3.7518686708062887\n",
            "Epoch: 35, Loss: 3.475503323599696\n",
            "Epoch: 36, Loss: 3.2161719910800457\n",
            "Epoch: 37, Loss: 2.981751497834921\n",
            "Epoch: 38, Loss: 2.7768356977030635\n",
            "Epoch: 39, Loss: 2.58904795255512\n",
            "Epoch: 40, Loss: 2.415048949420452\n",
            "Epoch: 41, Loss: 2.2551002874970436\n",
            "Epoch: 42, Loss: 2.1081029381603003\n",
            "Epoch: 43, Loss: 1.9779156520962715\n",
            "Epoch: 44, Loss: 1.8532678680494428\n",
            "Epoch: 45, Loss: 1.7372724311426282\n",
            "Epoch: 46, Loss: 1.6341347377747297\n",
            "Epoch: 47, Loss: 1.5364631400443614\n",
            "Epoch: 48, Loss: 1.4480162835679948\n",
            "Epoch: 49, Loss: 1.3629172835499048\n",
            "Epoch: 50, Loss: 1.2839755490422249\n",
            "Epoch: 51, Loss: 1.211232888046652\n",
            "Epoch: 52, Loss: 1.143904640339315\n",
            "Epoch: 53, Loss: 1.0797228207811713\n",
            "Epoch: 54, Loss: 1.0190385361202061\n",
            "Epoch: 55, Loss: 0.9657021574676037\n",
            "Epoch: 56, Loss: 0.9112164317630231\n",
            "Epoch: 57, Loss: 0.8633187138475478\n",
            "Epoch: 58, Loss: 0.8166406373493373\n",
            "Epoch: 59, Loss: 0.774506906978786\n",
            "Epoch: 60, Loss: 0.7338531438726932\n",
            "Epoch: 61, Loss: 0.6964911282993853\n",
            "Epoch: 62, Loss: 0.6593126766383648\n",
            "Epoch: 63, Loss: 0.626373345265165\n",
            "Epoch: 64, Loss: 0.5945000140927732\n",
            "Epoch: 65, Loss: 0.5642881288658828\n",
            "Epoch: 66, Loss: 0.5357764577493072\n",
            "Epoch: 67, Loss: 0.5088149721268564\n",
            "Epoch: 68, Loss: 0.48391167470254004\n",
            "Epoch: 69, Loss: 0.4597243673633784\n",
            "Epoch: 70, Loss: 0.43706976319663227\n",
            "Epoch: 71, Loss: 0.41555897635407746\n",
            "Epoch: 72, Loss: 0.39496146130841225\n",
            "Epoch: 73, Loss: 0.37625918607227504\n",
            "Epoch: 74, Loss: 0.35828073299489915\n",
            "Epoch: 75, Loss: 0.34061006421688944\n",
            "Epoch: 76, Loss: 0.3247964840847999\n",
            "Epoch: 77, Loss: 0.3089707406470552\n",
            "Epoch: 78, Loss: 0.29407893551979214\n",
            "Epoch: 79, Loss: 0.2800491381203756\n",
            "Epoch: 80, Loss: 0.26665759773459285\n",
            "Epoch: 81, Loss: 0.25459955423139036\n",
            "Epoch: 82, Loss: 0.24255572352558374\n",
            "Epoch: 83, Loss: 0.23098683927673846\n",
            "Epoch: 84, Loss: 0.21971788280643523\n",
            "Epoch: 85, Loss: 0.20983011624775827\n",
            "Epoch: 86, Loss: 0.19976580562070012\n",
            "Epoch: 87, Loss: 0.19062712893355638\n",
            "Epoch: 88, Loss: 0.18192182370694354\n",
            "Epoch: 89, Loss: 0.17344038689043373\n",
            "Epoch: 90, Loss: 0.16565836139488965\n",
            "Epoch: 91, Loss: 0.15778036549454555\n",
            "Epoch: 92, Loss: 0.1505401969770901\n",
            "Epoch: 93, Loss: 0.14349538937676698\n",
            "Epoch: 94, Loss: 0.13692235434427857\n",
            "Epoch: 95, Loss: 0.1307500689290464\n",
            "Epoch: 96, Loss: 0.12471421132795513\n",
            "Epoch: 97, Loss: 0.11911057570250705\n",
            "Epoch: 98, Loss: 0.11397930094972253\n",
            "Epoch: 99, Loss: 0.10848387348232791\n",
            "Epoch: 100, Loss: 0.10359376441920176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(model, question, threshold = 0.5):\n",
        "    num_question = text_to_indices(question, vocab)\n",
        "    tensor_question = torch.tensor(num_question).unsqueeze(0)\n",
        "    output = model(tensor_question)\n",
        "    probs = nn.Softmax(dim = 1)(output)\n",
        "    value, index = torch.max(probs, dim = 1)\n",
        "    print(value, index)"
      ],
      "metadata": {
        "id": "9tJ1wWqugzVC"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction(model, \"What is campusx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTrFyKYVhgx4",
        "outputId": "af28f2a6-3989-44a8-a9f4-d4bea0390a8c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1886], grad_fn=<MaxBackward0>) tensor([34])\n"
          ]
        }
      ]
    }
  ]
}